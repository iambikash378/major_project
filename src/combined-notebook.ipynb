{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10551997,"sourceType":"datasetVersion","datasetId":6528864},{"sourceId":10907789,"sourceType":"datasetVersion","datasetId":6780071},{"sourceId":10910661,"sourceType":"datasetVersion","datasetId":6782320},{"sourceId":10921462,"sourceType":"datasetVersion","datasetId":6789971}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install mlflow dagshub -qqq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:46:10.748986Z","iopub.execute_input":"2025-03-04T16:46:10.749380Z","iopub.status.idle":"2025-03-04T16:46:14.735516Z","shell.execute_reply.started":"2025-03-04T16:46:10.749347Z","shell.execute_reply":"2025-03-04T16:46:14.734469Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"import mlflow\nimport dagshub\n\ndagshub.init(\n    repo_name=\"mlflow_major\",  \n    repo_owner=\"iambikash378\", \n    mlflow=True               \n)\n\nmlflow.set_tracking_uri(f\"https://dagshub.com/iambikash378/mlflow_major.mlflow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:46:17.027133Z","iopub.execute_input":"2025-03-04T16:46:17.027481Z","iopub.status.idle":"2025-03-04T16:46:17.191901Z","shell.execute_reply.started":"2025-03-04T16:46:17.027451Z","shell.execute_reply":"2025-03-04T16:46:17.191020Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"iambikash378/mlflow_major\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"iambikash378/mlflow_major\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository iambikash378/mlflow_major initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository iambikash378/mlflow_major initialized!\n</pre>\n"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"import os\nos.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"iambikash378\"\nos.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"0bc681204678038909d3a123144d2dc73900d017\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:46:19.060656Z","iopub.execute_input":"2025-03-04T16:46:19.061037Z","iopub.status.idle":"2025-03-04T16:46:19.065821Z","shell.execute_reply.started":"2025-03-04T16:46:19.061007Z","shell.execute_reply":"2025-03-04T16:46:19.064844Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch import optim, nn\nfrom torch.utils.data import DataLoader\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchvision import models\nfrom torch.nn.functional import relu\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T16:46:20.826385Z","iopub.execute_input":"2025-03-04T16:46:20.826730Z","iopub.status.idle":"2025-03-04T16:46:20.831721Z","shell.execute_reply.started":"2025-03-04T16:46:20.826700Z","shell.execute_reply":"2025-03-04T16:46:20.830834Z"}},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":"## Loss Functions\n","metadata":{}},{"cell_type":"markdown","source":"### Binary Cross Entropy Loss","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef bce_loss(y_pred, y_true, smooth = 1e-6):\n    y_pred = y_pred.float().view(-1)\n    y_true = y_true.float().view(-1)\n\n    # Binary cross entropy calculation\n    bce = - (y_true * torch.log(y_pred + smooth) + (1 - y_true) * torch.log(1 - y_pred + smooth)).mean()\n\n    return bce\n\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:17:23.583242Z","iopub.execute_input":"2025-03-04T15:17:23.583576Z","iopub.status.idle":"2025-03-04T15:17:23.588717Z","shell.execute_reply.started":"2025-03-04T15:17:23.583550Z","shell.execute_reply":"2025-03-04T15:17:23.587715Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Dice Loss and Dice Score","metadata":{}},{"cell_type":"code","source":"\ndef dice_loss(y_pred, y_true, smooth = 1e-6):\n    y_pred = y_pred.float().view(-1)\n    y_true = y_true.float().view(-1)\n\n    # print(y_pred.shape, y_true.shape)\n    # print(y_pred.min(), y_pred.max())  # Should be between 0 and 1 (after sigmoid)\n    # print(y_true.min(), y_true.max())  # Should be 0 or 1\n\n    intersection = (y_pred * y_true).sum()\n    union = y_pred.sum()+y_true.sum()\n    if ((2.0*intersection)+smooth) > (union+smooth):\n        print(\"Error !\")\n    dice = ((2.0 * intersection) + smooth) / (union + smooth)\n\n    return 1-dice\n\ndef dice_score(y_pred, y_true):\n    y_pred = torch.sigmoid(y_pred)\n    y_pred = (y_pred > 0.5).float()\n    intersection = (y_pred * y_true).sum()\n    return (2.0 * intersection + 1e-6) / (y_pred.sum() + y_true.sum() + 1e-6) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T17:10:26.672271Z","iopub.execute_input":"2025-03-04T17:10:26.672559Z","iopub.status.idle":"2025-03-04T17:10:26.678059Z","shell.execute_reply.started":"2025-03-04T17:10:26.672537Z","shell.execute_reply":"2025-03-04T17:10:26.677145Z"}},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":"### Focal Loss","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n\n    def forward(self, preds, targets):\n        bce_loss = self.bce(preds, targets)\n        p_t = torch.sigmoid(preds) * targets + (1 - torch.sigmoid(preds)) * (1 - targets)\n        focal_weight = self.alpha * (1 - p_t) ** self.gamma\n        focal_loss = focal_weight * bce_loss\n        return focal_loss.mean()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Lovasz Loss","metadata":{}},{"cell_type":"code","source":"from torch.autograd import Variable\n\nclass LovaszHingeLoss(nn.Module):\n    def __init__(self):\n        super(LovaszHingeLoss, self).__init__()\n\n    def forward(self, preds, targets):\n        preds = torch.sigmoid(preds)  # Convert logits to probabilities\n        errors = (1 - 2 * targets) * preds  # Compute errors\n        sorted_errors, indices = torch.sort(errors, dim=0, descending=True)\n        grad = torch.linspace(1, 0, sorted_errors.numel()).to(preds.device)\n        loss = torch.dot(F.relu(sorted_errors), grad)\n        return loss.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dice Loss + BCE Loss (Balances Region Overlap and Pixel Wise Classification)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1e-6):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, preds, targets):\n\n        preds = torch.sigmoid(preds)\n\n        preds_flat = preds.view(-1)\n        targets_flat = targets.view(-1)\n\n        intersection = torch.sum(preds_flat * targets_flat)\n        union = torch.sum(preds_flat) + torch.sum(targets_flat)\n\n        dice_score = (2. * intersection + self.smooth) / (union + intersection + self.smooth)\n\n        dice_loss = 1 - dice_score\n        return dice_loss\n\n\nclass ComboLoss(nn.Module):\n    def __init__(self, bce_weight=0.25, dice_weight=0.75):\n        super(ComboLoss, self).__init__()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.dice = DiceLoss()\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n\n    def forward(self, preds, targets):\n        bce_loss = self.bce(preds, targets)\n        dice_loss = self.dice(preds, targets)\n        return self.bce_weight * bce_loss + self.dice_weight * dice_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:17:27.800335Z","iopub.execute_input":"2025-03-04T15:17:27.800692Z","iopub.status.idle":"2025-03-04T15:17:27.807787Z","shell.execute_reply.started":"2025-03-04T15:17:27.800660Z","shell.execute_reply":"2025-03-04T15:17:27.806788Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### Lovasz + Dice Loss","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef dice_loss(pred, target, smooth=1e-6):\n    \"\"\"Computes Dice loss.\"\"\"\n    pred = pred.view(-1)  # Flatten\n    target = target.view(-1)  # Flatten\n    \n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    \n    return 1 - dice  # Dice loss (1 - Dice coefficient)\n\ndef lovasz_hinge(logits, labels):\n    \"\"\"Lov치sz hinge loss for binary segmentation.\"\"\"\n    def lovasz_grad(gt_sorted):\n        \"\"\"Computes gradient of Lov치sz extension.\"\"\"\n        gts = gt_sorted.sum()\n        intersection = gts - gt_sorted.float().cumsum(0)\n        union = gts + (1 - gt_sorted).float().cumsum(0)\n        jaccard = 1. - intersection / union\n        jaccard[1:] = jaccard[1:] - jaccard[:-1]\n        return jaccard\n\n    logits = logits.view(-1)\n    labels = labels.view(-1)\n\n    signs = 2. * labels - 1.  # Convert {0,1} labels to {-1,1}\n    errors = 1. - logits * signs\n    errors_sorted, perm = torch.sort(errors, descending=True)\n    gt_sorted = labels[perm]\n    \n    return torch.dot(F.relu(errors_sorted), lovasz_grad(gt_sorted))\n\nclass CombinedLoss(nn.Module):\n    \"\"\"Combined Lov치sz + Dice Loss for highly imbalanced segmentation.\"\"\"\n    def __init__(self, dice_weight=0.5, lovasz_weight=0.5):\n        super().__init__()\n        self.dice_weight = dice_weight\n        self.lovasz_weight = lovasz_weight\n\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)  # Convert logits to probabilities\n\n        dice = dice_loss(probs, targets)\n        lovasz = lovasz_hinge(logits, targets)  # Use logits directly for Lov치sz hinge loss\n\n        return self.dice_weight * dice + self.lovasz_weight * lovasz\n\n# Usage in training loop:\ncriterion = CombinedLoss(dice_weight=0.5, lovasz_weight=0.5)\n\n# Example usage:\nlogits = torch.randn(2, 1, 128, 128)  # Example model output\ntargets = torch.randint(0, 2, (2, 1, 128, 128)).float()  # Binary ground truth mask\n\nloss = criterion(logits, targets)\nprint(\"Loss:\", loss.item())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:57:27.496461Z","iopub.execute_input":"2025-03-04T19:57:27.496823Z","iopub.status.idle":"2025-03-04T19:57:27.530242Z","shell.execute_reply.started":"2025-03-04T19:57:27.496795Z","shell.execute_reply":"2025-03-04T19:57:27.529287Z"}},"outputs":[{"name":"stdout","text":"Loss: 0.9681923389434814\n","output_type":"stream"}],"execution_count":183},{"cell_type":"markdown","source":"## Dataset Loader\n","metadata":{}},{"cell_type":"code","source":"\nclass hrgldd_dataset(Dataset): \n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def __len__(self):\n        return len(self.x)\n      \n    def __getitem__(self, index):\n        img = self.x[index]\n        label = self.y[index]\n\n        img = torch.from_numpy(img).float().permute(2,0,1)\n        label = torch.from_numpy(label).float().permute(2,0,1)\n\n        return img, label\n\npath_to_testX = '/kaggle/input/hrgldd-data/testX.npy'\npath_to_testY = '/kaggle/input/hrgldd-data/testY.npy'\npath_to_trainX = '/kaggle/input/augmented-training/jointX.npy'\npath_to_trainY = '/kaggle/input/augmented-training/jointY.npy'\npath_to_valX = '/kaggle/input/hrgldd-data/valX.npy'\npath_to_valY = '/kaggle/input/hrgldd-data/valY.npy'\n\ndata_testY = np.load(path_to_testY)\ndata_testX = np.load(path_to_testX)\ndata_trainX = np.load(path_to_trainX)\ndata_trainY = np.load(path_to_trainY)\ndata_valX = np.load(path_to_valX)\ndata_valY = np.load(path_to_valY)\n\nprint(data_trainY.shape)\nprint(data_trainX.shape)\nprint(data_trainX.dtype)\nprint(data_trainY.dtype)\nprint(data_valX.shape)\nprint(data_valY.shape)\n\n\ntrain_dataset = hrgldd_dataset(data_trainX, data_trainY)\nim, lbl = train_dataset[0]\n\nprint(im.shape)\n#print(lbl)\nprint(max(lbl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:59:19.743056Z","iopub.execute_input":"2025-03-04T19:59:19.743413Z","iopub.status.idle":"2025-03-04T19:59:20.271703Z","shell.execute_reply.started":"2025-03-04T19:59:19.743379Z","shell.execute_reply":"2025-03-04T19:59:20.270622Z"}},"outputs":[{"name":"stdout","text":"(3357, 128, 128, 1)\n(3357, 128, 128, 4)\nfloat32\nfloat32\n(284, 128, 128, 4)\n(284, 128, 128, 1)\ntorch.Size([4, 128, 128])\ntensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n","output_type":"stream"}],"execution_count":184},{"cell_type":"markdown","source":"## Dataset Loader fo OnTheFlyAugmentation only 3 Bands","metadata":{}},{"cell_type":"code","source":"# path_to_testX_otf = '/kaggle/input/hrgldd-data/testX.npy'\n# path_to_testY_otf = '/kaggle/input/hrgldd-data/testY.npy'\n# path_to_trainX_otf = '/kaggle/input/hrgldd-data/trainX.npy'\n# path_to_trainY_otf = '/kaggle/input/hrgldd-data/trainY.npy'\n# path_to_valX_otf = '/kaggle/input/hrgldd-data/valX.npy'\n# path_to_valY_otf = '/kaggle/input/hrgldd-data/valY.npy'\n\n# data_testY_otf = np.load(path_to_testY)\n# data_testX_otf = np.load(path_to_testX)\n# data_trainX_otf = np.load(path_to_trainX)\n# data_trainY_otf = np.load(path_to_trainY)\n# data_valX_otf = np.load(path_to_valX)\n# data_valY_otf = np.load(path_to_valY)\n\n\n# import torch\n# from torch.utils.data import Dataset\n# from torchvision import transforms\n# import random\n# from PIL import Image\n\n# class HRGLDDataset_onthefly(Dataset):\n#     def __init__(self, x, y, transform=None, label_transform=None):\n#         self.x = x  # BGR + NIR images\n#         self.y = y  # Labels (e.g., masks)\n#         self.transform = transform  # Optional transformation for images\n#         self.label_transform = label_transform  # Optional transformation for labels\n\n#     def __len__(self):\n#         return len(self.x)\n\n#     def __getitem__(self, index):\n#         img = self.x[index]\n#         label = self.y[index]\n\n#         img = img[..., :3]\n\n#         img = np.clip(img, 0, 1) * 255  # If the image is normalized, clip and scale\n#         img = img.astype(np.uint8)  # Convert to uint8 (necessary for PIL)\n#         # Convert images to PIL Image (since many transforms work on PIL images)\n#         img = Image.fromarray(img[..., ::-1])\n\n#         label = label.squeeze()  # Remove any unnecessary dimensions, making it 2D (H, W)\n#         # Convert label to uint8 if it's a binary mask\n#         label = (label * 255).astype(np.uint8)  # Scale to 0 or 255 for masks\n#         label = Image.fromarray(label)  # Assuming label is also in numpy format, convert to PIL\n\n#         # Apply image transformations (color jitter, flipping, rotation, etc.)\n#         if self.transform:\n#             img = self.transform(img)  \n        \n#         if self.label_transform:\n#             label = self.label_transform(label)  # Apply transformations to the label (mask)\n\n#         # Convert to tensor and permute to match (C, H, W) format for PyTorch\n#         img = torch.from_numpy(np.array(img)).float().permute(2, 0, 1)\n#         img = img/255.0# BGR + NIR to CxHxW\n        \n        \n#         label = torch.from_numpy(np.array(label)).float().unsqueeze(0)  # Mask to CxHxW format (1 channel)\n#         label = label/255.0\n\n#         return img, label\n\n# # Define image transformation pipeline (for BGR + NIR)\n# image_transform = transforms.Compose([\n#     transforms.RandomHorizontalFlip(),  # Random horizontal flip\n#     transforms.RandomVerticalFlip(),    # Random vertical flip\n#     transforms.RandomRotation(30),      # Random rotation between -30 to 30 degrees\n#     transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # Random cropping and scaling (crop size: 128x128)\n#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color jitter\n# ])\n\n# # Define label transformation pipeline (spatial transformations)\n# label_transform = transforms.Compose([\n#     transforms.RandomHorizontalFlip(),  # Random horizontal flip for the label\n#     transforms.RandomVerticalFlip(),    # Random vertical flip for the label\n#     transforms.RandomRotation(30),      # Random rotation for the label\n#     transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # Random cropping and scaling for the label\n# ])\n\n# # Instantiate the dataset with both image and label transformations\n# train_dataset_otf = HRGLDDataset_onthefly(data_trainX_otf, data_trainY_otf, transform=image_transform, label_transform=label_transform)\n# val_dataset_otf = hrgldd_dataset(data_valX_otf, data_valY_otf)\n\n# i, l = train_dataset_otf[100]\n# print(torch.max(l))\n# print(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:00:33.182421Z","iopub.execute_input":"2025-03-04T19:00:33.182804Z","iopub.status.idle":"2025-03-04T19:00:33.609776Z","shell.execute_reply.started":"2025-03-04T19:00:33.182743Z","shell.execute_reply":"2025-03-04T19:00:33.608804Z"}},"outputs":[{"name":"stdout","text":"tensor(1.)\n3357\n","output_type":"stream"}],"execution_count":165},{"cell_type":"markdown","source":"## Model (UNET)","metadata":{}},{"cell_type":"code","source":"\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv,self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(out_channels, out_channels,kernel_size = 3, padding = 1),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n    \n\nclass DownSample(nn.Module):\n    \n    def __init__(self, in_channels, out_channels):\n        super(DownSample, self).__init__()\n        self.conv = DoubleConv(in_channels, out_channels)\n        self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n    def forward(self,x):\n        down = self.conv(x)\n        p = self.pool(down)\n        return down, p\n\n\nclass UpSample(nn.Module):\n    \n    def __init__(self,in_channels,out_channels):\n        super(UpSample,self).__init__()\n        self.up = nn.ConvTranspose2d(in_channels, in_channels//2,\n                                     kernel_size = 2,\n                                     stride = 2)\n        self.conv = DoubleConv(in_channels,out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        x = torch.cat([x1,x2],dim = 1)\n        x2 = self.conv(x)\n        return x2\n    \n    \nclass UNet(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(UNet, self).__init__()\n\n        self.down_conv_1 = DownSample(in_channels, 64)\n        self.down_conv_2 = DownSample(64, 128)\n        self.down_conv_3 = DownSample(128, 256) \n        self.down_conv_4 = DownSample(256, 512)\n\n        self.bottleneck = DoubleConv(512, 1024)\n\n        self.up_conv_1 = UpSample(1024,512)\n        self.up_conv_2 = UpSample(512,256)\n        self.up_conv_3 = UpSample(256,128)\n        self.up_conv_4 = UpSample(128,64)\n\n        self.out = nn.Conv2d(in_channels=64, out_channels = 1,\n                             kernel_size = 3, padding = 1\n                             )\n\n    def forward(self,x):\n        #print(f\"Input Shape : {x.shape}\")\n        \n        down1, p1 = self.down_conv_1(x)\n        #print(f\"Shape after doubl_conv_1_only : {down1.shape}\")\n        #print(f\"Shape after down_conv_1 : {p1.shape}\")\n        \n        down2, p2 = self.down_conv_2(p1)\n        #print(f\"Shape after doubl_conv_2_only : {down2.shape}\")\n        #print(f\"Shape after down_conv_2: {p2.shape}\")\n        \n        down3, p3 = self.down_conv_3(p2)\n        #print(f\"Shape after doubl_conv_3_only : {down3.shape}\")\n        #print(f\"Shape after down_conv_3 : {p3.shape}\")\n        \n        down4, p4 = self.down_conv_4(p3)\n        #print(f\"Shape after doubl_conv_4_only : {down4.shape}\")\n        #print(f\"Shape after down_conv_4 : {p4.shape}\")\n\n        b = self.bottleneck(p4)\n        #print(f\"Shape after bottleneck : {b.shape}\")\n\n        up_1 = self.up_conv_1(b, down4)\n        #print(f\"Shape after up_1 : {up_1.shape}\")\n        up_2 = self.up_conv_2(up_1,down3)\n        #print(f\"Shape after up_2 : {up_2.shape}\")\n        up_3 = self.up_conv_3(up_2, down2)\n        #print(f\"Shape after up_3 : {up_3.shape}\")\n        up_4 = self.up_conv_4(up_3, down1)\n        #print(f\"Shape after up_4 : {up_4.shape}\")\n\n        op = self.out(up_4)\n        #print(f\"Shape of output : {op.shape}\")\n\n        return op\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T19:59:28.197661Z","iopub.execute_input":"2025-03-04T19:59:28.198050Z","iopub.status.idle":"2025-03-04T19:59:28.209454Z","shell.execute_reply.started":"2025-03-04T19:59:28.198022Z","shell.execute_reply":"2025-03-04T19:59:28.208480Z"}},"outputs":[],"execution_count":185},{"cell_type":"markdown","source":"## Train\n\n### Training Loop for UNet with 64 Filters","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 5e-4\nBATCH_SIZE = 32\nEPOCHS = 500\nPATIENCE = 20\nDATA_PATH = \"/kaggle/input/hrgldd-data\"\nMODEL_SAVE_PATH = \"/kaggle/working/best_model.pth\"\n\nmodel_dir = os.path.dirname(MODEL_SAVE_PATH)\nif not os.path.exists(model_dir):\n    print(f\"Creating directory: {model_dir}\")\n    os.makedirs(model_dir, exist_ok=True)\nelse:\n    print(f\"Directory already exists: {model_dir}\")\n\nval_loss_list = []\ntrain_loss_list = []\nf1_score_list = []\nrecall_list = []\naccuracy_list = []\nprecision_list = []\n\ncriterion = CombinedLoss(dice_weight=0.5, lovasz_weight=0.5)\n\n\ntrain_dataset = hrgldd_dataset(data_trainX, data_trainY)\nval_dataset = hrgldd_dataset(data_valX, data_valY)\ntest_dataset = hrgldd_dataset(data_testX, data_testY)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                               batch_size=BATCH_SIZE,\n                               shuffle=True)\nval_dataloader = DataLoader(dataset=val_dataset,\n                             batch_size=BATCH_SIZE,\n                             shuffle=True)\n\nmodel = UNet(4, 1).to(device)\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n\nbest_val_loss = float('inf')\n\nwith mlflow.start_run():\n    mlflow.log_params(\n        {\n            \"initial_learning_rate\": LEARNING_RATE,\n            \"batch_size\": BATCH_SIZE,\n            \"model_arch\": 'UNet',\n            \"loss_function\": 'combined lov dice',\n            \"epochs\": EPOCHS,\n            \"patience\": PATIENCE,\n            \"filters\": [64, 128, 256, 512, 1024],\n            \"optimizer\": type(optimizer).__name__,\n            \"threshold\" : 0.5,\n            \"remarks\" : \"augmented data\"\n        }\n    )\n\n    for epoch in range(EPOCHS):\n        train_running_loss = 0\n        model.train()\n        for index, x_y in enumerate(train_dataloader):\n            img = x_y[0].float().to(device)\n            mask = x_y[1].float().to(device)\n\n            optimizer.zero_grad()\n            outputs = model(img)\n            train_loss = criterion(outputs, mask)\n            train_loss.backward()  # Find Gradients by Backward Pass\n            optimizer.step()  # Update the weights\n\n            train_running_loss += train_loss.item()\n\n        train_loss = train_running_loss / len(train_dataloader)\n        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n\n        train_loss_list.append(train_loss)\n\n        # --------------- Validation Part ------------------------ #\n\n        model.eval()\n        val_running_loss = 0\n        #val_dice = 0\n        val_accuracy = 0\n        val_precision = 0\n        val_recall = 0\n        val_f1 = 0\n        with torch.no_grad():\n            for index, x_y in enumerate(val_dataloader):\n                img = x_y[0].float().to(device)\n                mask = x_y[1].float().to(device)\n\n                y_pred = model(img)\n                val_loss = criterion(y_pred, mask)\n                val_running_loss += val_loss.item()\n\n\n                y_pred_binary = (torch.sigmoid(y_pred) > 0.6).float()\n                mask_binary = (mask > 0.5).float()\n                y_pred_flat = y_pred_binary.view(-1).cpu().numpy()\n                mask_flat = mask_binary.view(-1).cpu().numpy()\n\n                val_accuracy += accuracy_score(mask_flat, y_pred_flat)\n                val_precision += precision_score(mask_flat, y_pred_flat, zero_division=0)\n                val_recall += recall_score(mask_flat, y_pred_flat, zero_division=0)\n                val_f1 += f1_score(mask_flat, y_pred_flat, zero_division=0)\n\n            val_loss = val_running_loss / len(val_dataloader)\n            val_loss_list.append(val_loss)\n            #val_dice = val_dice / len(val_dataloader)\n            val_accuracy = val_accuracy / len(val_dataloader)\n            accuracy_list.append(val_accuracy)\n            val_precision = val_precision / len(val_dataloader)\n            precision_list.append(val_precision)\n            val_recall = val_recall / len(val_dataloader)\n            recall_list.append(val_recall)\n            val_f1 = val_f1 / len(val_dataloader)\n            f1_score_list.append(val_f1)\n\n            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n            mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n            mlflow.log_metric(\"val_precision\", val_precision, step=epoch)\n            mlflow.log_metric(\"val_recall\", val_recall, step=epoch)\n            mlflow.log_metric(\"val_f1\", val_f1, step=epoch)\n\n        print(\"--\" * 30)\n        print(f\"Train Loss EPOCH {epoch + 1}: {train_loss:.4f}\")\n        print(f\"Val Loss EPOCH {epoch + 1} : {val_loss:.4f}\")\n        #print(f\"Val Dice Score EPOCH {epoch + 1} : {val_dice:.4f}\")\n        print(f\"Val Precision EPOCH {epoch + 1}: {val_precision:.4f}\")\n        print(f\"Val Recall EPOCH {epoch + 1}: {val_recall:.4f}\")\n        print(f\"Val F1 Score EPOCH {epoch + 1}: {val_f1:.4f}\")\n        print(f\"Current Learning Rate : {optimizer.param_groups[0]['lr']:.6f}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            counter = 0\n            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n            mlflow.log_artifact(MODEL_SAVE_PATH)\n            print(f\"Saved best model with val loss : {val_loss:.4f}\")\n        else:\n            counter += 1\n            print(f\"No improvement in validation loss for {counter} epochs.\")\n\n        if counter >= PATIENCE:\n            print(f\"Early stopping triggered after {PATIENCE} epochs without improvement.\")\n            break\n\n        scheduler.step(val_loss)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = UNet(in_channels=4, out_channels=1).to(device)\n\nx = torch.randn(16, 4, 128, 128).to(device)  # Batch of 16 images, 4 channels, 256x256 resolution\noutput = model(x)\nprint(output.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T05:08:03.265368Z","iopub.execute_input":"2025-01-25T05:08:03.265608Z","iopub.status.idle":"2025-01-25T05:08:03.595973Z","shell.execute_reply.started":"2025-01-25T05:08:03.265589Z","shell.execute_reply":"2025-01-25T05:08:03.595134Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 1, 128, 128])\n","output_type":"stream"}],"execution_count":8}]}